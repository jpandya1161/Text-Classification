{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "9iW8NgJ4SrrI"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "JrJSEgNhSqUm"
      },
      "outputs": [],
      "source": [
        "# Define the paths\n",
        "train_dir = '/content/datasets/train'\n",
        "test_dir = '/content/datasets/test'\n",
        "test_data_dir = '/content/test_data'\n",
        "\n",
        "# Create the directories (including any parent directories)\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "os.makedirs(test_data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/datasets/train/ham /content/datasets/train/spam\n",
        "!mkdir -p /content/datasets/test/ham /content/datasets/test/spam\n",
        "!mkdir -p /content/test_data/enron1 /content/test_data/enron2 /content/test_data/enron4"
      ],
      "metadata": {
        "id": "HNOqBeWFT7U6"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/project1_datasets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cR5JBvPS0rF",
        "outputId": "ddb0ad23-b18c-4b25-bf62-511b9aa768c3",
        "collapsed": true
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/project1_datasets.zip\n",
            "replace project1_datasets/enron4_test.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: project1_datasets/enron4_test.zip  \n",
            "  inflating: __MACOSX/project1_datasets/._enron4_test.zip  \n",
            "  inflating: project1_datasets/enron1_train.zip  \n",
            "  inflating: __MACOSX/project1_datasets/._enron1_train.zip  \n",
            "  inflating: project1_datasets/enron2_test.zip  \n",
            "  inflating: __MACOSX/project1_datasets/._enron2_test.zip  \n",
            "  inflating: project1_datasets/enron2_train.zip  \n",
            "  inflating: __MACOSX/project1_datasets/._enron2_train.zip  \n",
            "  inflating: project1_datasets/enron4_train.zip  \n",
            "  inflating: __MACOSX/project1_datasets/._enron4_train.zip  \n",
            "  inflating: project1_datasets/enron1_test.zip  \n",
            "  inflating: __MACOSX/project1_datasets/._enron1_test.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Step 1: Create directories for extraction and final locations\n",
        "os.makedirs('/content/datasets/temp/train', exist_ok=True)\n",
        "os.makedirs('/content/datasets/temp/test', exist_ok=True)\n",
        "os.makedirs('/content/datasets/train/ham', exist_ok=True)\n",
        "os.makedirs('/content/datasets/train/spam', exist_ok=True)\n",
        "\n",
        "# Step 2: Unzip enron1, enron2, enron4 into the temp directory\n",
        "def extract_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "extract_zip('/content/project1_datasets/enron1_train.zip', '/content/datasets/temp')\n",
        "extract_zip('/content/project1_datasets/enron2_train.zip', '/content/datasets/temp')\n",
        "extract_zip('/content/project1_datasets/enron4_train.zip', '/content/datasets/temp')\n",
        "\n",
        "# Step 3: Move ham and spam files to the correct locations\n",
        "def move_files(source, dest):\n",
        "    if os.path.exists(source):\n",
        "        for file in os.listdir(source):\n",
        "            os.rename(os.path.join(source, file), os.path.join(dest, file))\n",
        "\n",
        "move_files('/content/datasets/temp/enron1/train/ham', '/content/datasets/train/ham')\n",
        "move_files('/content/datasets/temp/enron1/train/spam', '/content/datasets/train/spam')\n",
        "move_files('/content/datasets/temp/train/ham', '/content/datasets/train/ham')\n",
        "move_files('/content/datasets/temp/train/spam', '/content/datasets/train/spam')\n",
        "move_files('/content/datasets/temp/enron4/train/ham', '/content/datasets/train/ham')\n",
        "move_files('/content/datasets/temp/enron4/train/spam', '/content/datasets/train/spam')\n",
        "\n",
        "# Step 4: Optionally clean up the temp directory after extraction\n",
        "import shutil\n",
        "shutil.rmtree('/content/datasets/temp')"
      ],
      "metadata": {
        "id": "X9XueB9VYmFE"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Step 1: Create directories for extraction and final locations\n",
        "os.makedirs('/content/datasets/temp/train', exist_ok=True)\n",
        "os.makedirs('/content/datasets/temp/test', exist_ok=True)\n",
        "os.makedirs('/content/datasets/test/ham', exist_ok=True)\n",
        "os.makedirs('/content/datasets/test/spam', exist_ok=True)\n",
        "\n",
        "os.makedirs('/content/test_data/enron1', exist_ok=True)\n",
        "os.makedirs('/content/test_data/enron2', exist_ok=True)\n",
        "os.makedirs('/content/test_data/enron4', exist_ok=True)\n",
        "\n",
        "# Step 2: Unzip enron1, enron2, enron4 into the temp directory\n",
        "def extract_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "extract_zip('/content/project1_datasets/enron1_test.zip', '/content/datasets/temp')\n",
        "extract_zip('/content/project1_datasets/enron2_test.zip', '/content/datasets/temp')\n",
        "extract_zip('/content/project1_datasets/enron4_test.zip', '/content/datasets/temp')\n",
        "\n",
        "# Step 3: Move ham and spam files to the correct locations\n",
        "def move_files(source, dest):\n",
        "    if os.path.exists(source):\n",
        "        for file in os.listdir(source):\n",
        "            os.rename(os.path.join(source, file), os.path.join(dest, file))\n",
        "\n",
        "shutil.copytree('/content/datasets/temp/enron1/test', '/content/test_data/enron1', dirs_exist_ok=True)\n",
        "shutil.copytree('/content/datasets/temp/test', '/content/test_data/enron2', dirs_exist_ok=True)\n",
        "shutil.copytree('/content/datasets/temp/enron4/test', '/content/test_data/enron4', dirs_exist_ok=True)\n",
        "\n",
        "move_files('/content/datasets/temp/enron1/test/ham', '/content/datasets/test/ham')\n",
        "move_files('/content/datasets/temp/enron1/test/spam', '/content/datasets/test/spam')\n",
        "move_files('/content/datasets/temp/test/ham', '/content/datasets/test/ham')\n",
        "move_files('/content/datasets/temp/test/spam', '/content/datasets/test/spam')\n",
        "move_files('/content/datasets/temp/enron4/test/ham', '/content/datasets/test/ham')\n",
        "move_files('/content/datasets/temp/enron4/test/spam', '/content/datasets/test/spam')\n",
        "\n",
        "# Step 4: Optionally clean up the temp directory after extraction\n",
        "import shutil\n",
        "shutil.rmtree('/content/datasets/temp')"
      ],
      "metadata": {
        "id": "7nVgTxQaZJzd"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def read_files_from_directory(directory, category):\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            try:\n",
        "                message = file.read().split(\"Subject:\", 1)[1].strip()  # Keep text after 'Subject:'\n",
        "            except IndexError:\n",
        "                message = file.read().strip()  # Fallback if no subject line\n",
        "            data.append((message, category))\n",
        "    return data\n",
        "\n",
        "# Paths to directories\n",
        "directories = {\n",
        "    'train_ham': '/content/datasets/train/ham',\n",
        "    'train_spam': '/content/datasets/train/spam',\n",
        "    'test_ham': '/content/datasets/test/ham',\n",
        "    'test_spam': '/content/datasets/test/spam',\n",
        "    'enron1_test_ham_data' : '/content/test_data/enron1/ham',\n",
        "    'enron1_test_spam_data' : '/content/test_data/enron1/spam',\n",
        "    'enron2_test_ham_data' : '/content/test_data/enron2/ham',\n",
        "    'enron2_test_spam_data' : '/content/test_data/enron2/spam',\n",
        "    'enron4_test_ham_data' : '/content/test_data/enron4/ham',\n",
        "    'enron4_test_spam_data' : '/content/test_data/enron4/spam',\n",
        "}\n",
        "\n",
        "# Read files and create DataFrames\n",
        "train_data = read_files_from_directory(directories['train_ham'], 'ham') + read_files_from_directory(directories['train_spam'], 'spam')\n",
        "test_data = read_files_from_directory(directories['test_ham'], 'ham') + read_files_from_directory(directories['test_spam'], 'spam')\n",
        "\n",
        "enron1_data = read_files_from_directory(directories['enron1_test_ham_data'], 'ham') + read_files_from_directory(directories['enron1_test_spam_data'], 'spam')\n",
        "enron2_data = read_files_from_directory(directories['enron2_test_ham_data'], 'ham') + read_files_from_directory(directories['enron2_test_spam_data'], 'spam')\n",
        "enron4_data = read_files_from_directory(directories['enron4_test_ham_data'], 'ham') + read_files_from_directory(directories['enron4_test_spam_data'], 'spam')\n",
        "\n",
        "# Create DataFrames\n",
        "train_df = pd.DataFrame(train_data, columns=['Message', 'Category'])\n",
        "test_df = pd.DataFrame(test_data, columns=['Message', 'Category'])\n",
        "\n",
        "enron1_df = pd.DataFrame(enron1_data, columns=['Message', 'Category'])\n",
        "enron2_df = pd.DataFrame(enron2_data, columns=['Message', 'Category'])\n",
        "enron4_df = pd.DataFrame(enron4_data, columns=['Message', 'Category'])"
      ],
      "metadata": {
        "id": "CWuJbGFgdjkr"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data: Convert 'ham' to 0 and 'spam' to 1, only once\n",
        "def preprocess_data(df):\n",
        "    if 'Category' in df.columns:\n",
        "        if df['Category'].dtype == object:  # Check if data is still in 'ham'/'spam' format\n",
        "            df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
        "    return df\n",
        "\n",
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)\n",
        "enron1_df = preprocess_data(enron1_df)\n",
        "enron2_df = preprocess_data(enron2_df)\n",
        "enron4_df = preprocess_data(enron4_df)"
      ],
      "metadata": {
        "id": "WRX0-lu0vJ0i"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multinomial Naive Bayes**"
      ],
      "metadata": {
        "id": "8GP6TYBMikRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bag of Words feature extraction using CountVectorizer\n",
        "v_bow = CountVectorizer()\n",
        "\n",
        "# Fit on training data and transform both train and test data\n",
        "df_bag_of_words = v_bow.fit_transform(train_df['Message'].values)"
      ],
      "metadata": {
        "id": "j8u_zc7Ef1KM"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = v_bow.get_feature_names_out()\n",
        "vocabulary_count = df_bag_of_words.shape[1]\n",
        "files_count = df_bag_of_words.shape[0]\n",
        "spam_files_count = train_df[train_df['Category'] == 1].shape[0]\n",
        "ham_files_count = train_df[train_df['Category'] == 0].shape[0]"
      ],
      "metadata": {
        "id": "qGC7jBwoiiBA"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_spam = CountVectorizer()\n",
        "spam_messages = train_df[train_df['Category'] == 1]['Message']\n",
        "spam_bag_of_words = v_spam.fit_transform(spam_messages.values)\n",
        "spam_terms = v_spam.get_feature_names_out()\n",
        "spam_word_freq = np.asarray(spam_bag_of_words.sum(axis=0)).flatten()\n",
        "spam_email_freq_of_words = dict(zip(spam_terms, spam_word_freq))"
      ],
      "metadata": {
        "id": "wJUForsMiOcz"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_ham = CountVectorizer()\n",
        "ham_messages = train_df[train_df['Category'] == 0]['Message']\n",
        "ham_bag_of_words = v_ham.fit_transform(ham_messages.values)\n",
        "ham_terms = v_ham.get_feature_names_out()\n",
        "ham_word_freq = np.asarray(ham_bag_of_words.sum(axis=0)).flatten()\n",
        "ham_email_freq_of_words = dict(zip(ham_terms, ham_word_freq))"
      ],
      "metadata": {
        "id": "HM1ZCkUFhtZ4"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_words_count = spam_bag_of_words.sum()\n",
        "ham_words_count = ham_bag_of_words.sum()"
      ],
      "metadata": {
        "id": "ZIzXMsRGi5cZ"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_multinomial_NB(spam_email_freq_of_words, ham_email_freq_of_words, spam_words_count, ham_words_count, files_count, spam_files_count, ham_files_count, vocabulary, vocabulary_count):\n",
        "    # Initialize dictionaries\n",
        "    prior = {}\n",
        "    conditional_probability = {\"spam\": {}, \"ham\": {}}\n",
        "\n",
        "    # Calculate prior probabilities\n",
        "    prior[\"spam\"] = np.log(spam_files_count / float(files_count))\n",
        "    prior[\"ham\"] = np.log(ham_files_count / float(files_count))\n",
        "\n",
        "    # Calculate conditional probabilities for each word in vocabulary\n",
        "    for word in vocabulary:\n",
        "        spam_word_count = spam_email_freq_of_words.get(word, 0)\n",
        "        ham_word_count = ham_email_freq_of_words.get(word, 0)\n",
        "\n",
        "        # Add-one Laplace smoothing\n",
        "        conditional_probability[\"spam\"][word] = np.log(\n",
        "            (spam_word_count + 1) / float(spam_words_count + vocabulary_count)\n",
        "        )\n",
        "        conditional_probability[\"ham\"][word] = np.log(\n",
        "            (ham_word_count + 1) / float(ham_words_count + vocabulary_count)\n",
        "        )\n",
        "\n",
        "    return prior, conditional_probability"
      ],
      "metadata": {
        "id": "iolsj7Og_mfR"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior, conditional_probability = train_multinomial_NB(\n",
        "    spam_email_freq_of_words, ham_email_freq_of_words,\n",
        "    spam_words_count, ham_words_count,\n",
        "    files_count, spam_files_count, ham_files_count,\n",
        "    vocabulary, vocabulary_count\n",
        ")"
      ],
      "metadata": {
        "id": "AvjzyvZ4AwyS"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def test_multinomial_NB(message, prior, conditional_probability, vocabulary):\n",
        "    # Tokenize the message\n",
        "    word_counts = Counter(message.split())\n",
        "\n",
        "    log_prob_spam = prior['spam']\n",
        "    log_prob_ham = prior['ham']\n",
        "\n",
        "    # Sum the log probabilities for each word based on its frequency\n",
        "    for word, count in word_counts.items():\n",
        "        if word in vocabulary:\n",
        "            # Multiply the word's conditional probability by the count (since we use log, we add them)\n",
        "            log_prob_spam += count * conditional_probability['spam'].get(word, 0)\n",
        "            log_prob_ham += count * conditional_probability['ham'].get(word, 0)\n",
        "\n",
        "    # Return the class with the highest log probability\n",
        "    return 1 if log_prob_spam > log_prob_ham else 0"
      ],
      "metadata": {
        "id": "OMxiXrrDe5W9"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_bag_of_words_test_data(df, prior, conditional_probability, vocabulary):\n",
        "    predictions = [test_multinomial_NB(message, prior, conditional_probability, vocabulary) for message in df['Message']]\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "TJKa0lwxK8Sg"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bag_of_words_naive_bayes(test_df, prior, conditional_probability, vocabulary, dataset_name):\n",
        "    # Predict and evaluate accuracy on test data\n",
        "    predictions = predict_bag_of_words_test_data(test_df, prior, conditional_probability, vocabulary)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_predictions = sum(pred == true for pred, true in zip(predictions, test_df['Category']))\n",
        "    accuracy = (correct_predictions / len(test_df)) * 100\n",
        "\n",
        "    print(f\"Test accuracy (Bag of Words Naive Bayes) on {dataset_name}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "Ng7GIZ3xLBSw"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function for different datasets\n",
        "evaluate_bag_of_words_naive_bayes(test_df, prior, conditional_probability, vocabulary, 'Test Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOE8HtbpLF_C",
        "outputId": "20c8e8e7-6c8c-474d-aba8-bfb7f542dba8"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy (Bag of Words Naive Bayes) on Test Dataset: 97.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discrete Naive Bayes**"
      ],
      "metadata": {
        "id": "5VTmO1EYiyYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bernoulli feature extraction using CountVectorizer\n",
        "v_bn = CountVectorizer(binary= True)\n",
        "\n",
        "# Fit on training data and transform both train and test data\n",
        "df_bernoulli = v_bn.fit_transform(train_df['Message'].values)"
      ],
      "metadata": {
        "id": "TOuuA2Tmi0zP"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = v_bn.get_feature_names_out()\n",
        "vocabulary_count = df_bernoulli.shape[1]\n",
        "files_count = df_bernoulli.shape[0]\n",
        "spam_files_count = train_df[train_df['Category'] == 1].shape[0]\n",
        "ham_files_count = train_df[train_df['Category'] == 0].shape[0]"
      ],
      "metadata": {
        "id": "bonat4jGjeYU"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_spam = CountVectorizer(binary= True)\n",
        "spam_messages = train_df[train_df['Category'] == 1]['Message']\n",
        "spam_word_presence = v_spam.fit_transform(spam_messages.values)\n",
        "spam_terms = v_spam.get_feature_names_out()\n",
        "spam_word_freq = np.asarray(spam_word_presence.sum(axis=0)).flatten()\n",
        "spam_email_freq_of_words = dict(zip(spam_terms, spam_word_freq))"
      ],
      "metadata": {
        "id": "JuMP_e38jn2S"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_ham = CountVectorizer(binary= True)\n",
        "ham_messages = train_df[train_df['Category'] == 0]['Message']\n",
        "ham_word_presence = v_ham.fit_transform(ham_messages.values)\n",
        "ham_terms = v_ham.get_feature_names_out()\n",
        "ham_word_freq = np.asarray(ham_word_presence.sum(axis=0)).flatten()\n",
        "ham_email_freq_of_words = dict(zip(ham_terms, ham_word_freq))"
      ],
      "metadata": {
        "id": "v5XgEH_ZjsIY"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_words_count = spam_word_presence.sum()\n",
        "ham_words_count = ham_word_presence.sum()"
      ],
      "metadata": {
        "id": "P-XXNRYRjuin"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_discrete_NB(spam_email_freq_of_words, ham_email_freq_of_words, spam_words_count, ham_words_count, files_count, spam_files_count, ham_files_count, vocabulary, vocabulary_count):\n",
        "    # Initialize dictionaries\n",
        "    prior = {}\n",
        "    conditional_probability = {\"spam\": {}, \"ham\": {}}\n",
        "    conditional_probability_of_not_seen_word = {\"spam\": {}, \"ham\": {}}\n",
        "\n",
        "    # Calculate prior probabilities\n",
        "    prior[\"spam\"] = np.log(spam_files_count / float(files_count))\n",
        "    prior[\"ham\"] = np.log(ham_files_count / float(files_count))\n",
        "\n",
        "    # Calculate conditional probabilities for each word in vocabulary\n",
        "    for word in vocabulary:\n",
        "        spam_word_count = spam_email_freq_of_words.get(word, 0)\n",
        "        ham_word_count = ham_email_freq_of_words.get(word, 0)\n",
        "\n",
        "        # Add-one Laplace smoothing\n",
        "        conditional_probability[\"spam\"][word] = np.log(\n",
        "            (spam_word_count + 1) / float(spam_words_count + 2)\n",
        "        )\n",
        "        conditional_probability[\"ham\"][word] = np.log(\n",
        "            (ham_word_count + 1) / float(ham_words_count + 2)\n",
        "        )\n",
        "\n",
        "    # Probability for words not seen in the training data\n",
        "    conditional_probability_of_not_seen_word[\"spam\"] = np.log(\n",
        "        1 / float(spam_words_count + 2)\n",
        "    )\n",
        "    conditional_probability_of_not_seen_word[\"ham\"] = np.log(\n",
        "        1 / float(ham_words_count + 2)\n",
        "    )\n",
        "\n",
        "    return prior, conditional_probability, conditional_probability_of_not_seen_word"
      ],
      "metadata": {
        "id": "Xnz1VvpFjxJq"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior, conditional_probability, conditional_probability_of_not_seen_word = train_discrete_NB(\n",
        "    spam_email_freq_of_words, ham_email_freq_of_words, spam_words_count, ham_words_count, files_count, spam_files_count, ham_files_count, vocabulary, vocabulary_count\n",
        ")"
      ],
      "metadata": {
        "id": "cuLBZou2rSqF"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_discrete_NB(message, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary):\n",
        "    # Tokenize the message\n",
        "    words = set(message.split())\n",
        "\n",
        "    # Initialize log probabilities for each class\n",
        "    log_prob_spam = prior[\"spam\"]\n",
        "    log_prob_ham = prior[\"ham\"]\n",
        "\n",
        "    # Calculate log probabilities for each class\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            log_prob_spam += conditional_probability[\"spam\"].get(word, conditional_probability_of_not_seen_word[\"spam\"])\n",
        "            log_prob_ham += conditional_probability[\"ham\"].get(word, conditional_probability_of_not_seen_word[\"ham\"])\n",
        "        else:\n",
        "            # If word is absent, Bernoulli treats it as 1 minus the presence probability\n",
        "            log_prob_spam += np.log(1 - np.exp(conditional_probability['spam'].get(word, conditional_probability_of_not_seen_word['spam'])))\n",
        "            log_prob_ham += np.log(1 - np.exp(conditional_probability['ham'].get(word, conditional_probability_of_not_seen_word['ham'])))\n",
        "\n",
        "    # Return the class with the highest log probability\n",
        "    return 1 if log_prob_spam > log_prob_ham else 0"
      ],
      "metadata": {
        "id": "pEHIuXVviH9h"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_bernoulli_test_data(df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary):\n",
        "    predictions = [test_discrete_NB(message, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary) for message in df['Message']]\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "EDUdATj8JZj8"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bernoulli_naive_bayes_on_dataset(test_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary, dataset_name):\n",
        "    # Predict and evaluate accuracy on test data\n",
        "    predictions = predict_bernoulli_test_data(test_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct_predictions = sum(pred == true for pred, true in zip(predictions, test_df['Category']))\n",
        "    accuracy = (correct_predictions / len(test_df)) * 100\n",
        "\n",
        "    print(f\"Final Test Accuracy (Bernoulli Naive Bayes) on {dataset_name}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "GjRXYXdYJIm1"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function for different datasets\n",
        "evaluate_bernoulli_naive_bayes_on_dataset(test_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary, 'Test Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20dHugiAJbcq",
        "outputId": "869e0c90-0306-41ac-95e3-6abbe299e4d2"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy (Bernoulli Naive Bayes) on Test Dataset: 95.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MCAP Logistic Regression**"
      ],
      "metadata": {
        "id": "cbCPuu6-VWbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "ebK-QAg6kI2I"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train_df into 70% train and 30% validation\n",
        "train_split, validation_split = train_test_split(train_df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "vfYtYHjgeCf0"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate sigmoid for logistic regression\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "gSmXEBGYeght"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression training function with MCAP regularization\n",
        "def mcap_logistic_regression_train(X, y, eta, lambda_parameter, num_iterations):\n",
        "    weights = np.zeros(X.shape[1])  # Initialize weights\n",
        "    for _ in range(num_iterations):\n",
        "        # Linear combination of inputs and weights\n",
        "        linear_model = X.dot(weights)\n",
        "        # Sigmoid activation to get probabilities\n",
        "        predictions = sigmoid(linear_model)\n",
        "\n",
        "        # Gradient for logistic regression with regularization\n",
        "        gradient = X.T.dot(y - predictions) / len(y) - lambda_parameter * weights\n",
        "        # Update rule for gradient ascent\n",
        "        weights += eta * gradient\n",
        "    return weights"
      ],
      "metadata": {
        "id": "5z0lHQfIeji-"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression testing function\n",
        "def mcap_logistic_regression_test(X, weights):\n",
        "    linear_model = X.dot(weights)\n",
        "    predictions = sigmoid(linear_model)\n",
        "    return np.where(predictions >= 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "o3RQx4i_eoMC"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation to find best lambda\n",
        "def find_best_lambda(X_train, y_train, X_val, y_val, eta, num_iterations):\n",
        "    best_lambda = 0\n",
        "    best_accuracy = 0\n",
        "    lambda_values = [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100]  # Hyperparameter tuning\n",
        "\n",
        "    for lambda_parameter in lambda_values:\n",
        "        weights = mcap_logistic_regression_train(X_train, y_train, eta, lambda_parameter, num_iterations)\n",
        "        predictions = mcap_logistic_regression_test(X_val, weights)\n",
        "        accuracy = np.mean(predictions == y_val)*100\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_lambda = lambda_parameter\n",
        "\n",
        "    return best_lambda, best_accuracy"
      ],
      "metadata": {
        "id": "xYq6cOtPeqYR"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.01  # Learning rate\n",
        "num_iterations = 1000  # Number of iterations"
      ],
      "metadata": {
        "id": "YLw-klx36gxh"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize text using CountVectorizer for Bag of Words\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_train_bow = vectorizer_bow.fit_transform(train_split['Message'])\n",
        "X_validation_bow = vectorizer_bow.transform(validation_split['Message'])"
      ],
      "metadata": {
        "id": "qN2Kiqmfg0uV"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_bow = train_split['Category']\n",
        "y_validation_bow = validation_split['Category']"
      ],
      "metadata": {
        "id": "B_oZZzpxhlUk"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "best_lambda_bow, best_accuracy_bow = find_best_lambda(X_train_bow, y_train_bow, X_validation_bow, y_validation_bow, eta, num_iterations)"
      ],
      "metadata": {
        "id": "opw9rrZhftsy"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Bag of Words Lambda: {best_lambda_bow}\")\n",
        "print(f\"Best Bag of words Validation Accuracy: {best_accuracy_bow:.2f}%\")"
      ],
      "metadata": {
        "id": "9ck_msG8gMvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45999513-9fee-4145-b678-27028528a5d7"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Bag of Words Lambda: 0\n",
            "Best Bag of words Validation Accuracy: 95.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize text using CountVectorizer for Bernoulli\n",
        "vectorizer_bn = CountVectorizer(binary=True)\n",
        "X_train_bn = vectorizer_bn.fit_transform(train_split['Message'])\n",
        "X_validation_bn = vectorizer_bn.transform(validation_split['Message'])"
      ],
      "metadata": {
        "id": "u_FnDXMQgzH2"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_bn = train_split['Category']\n",
        "y_validation_bn = validation_split['Category']"
      ],
      "metadata": {
        "id": "qVlByJUk3rA2"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning\n",
        "best_lambda_bn, best_accuracy_bn = find_best_lambda(X_train_bn, y_train_bn, X_validation_bn, y_validation_bn, eta, num_iterations)"
      ],
      "metadata": {
        "id": "lcUFSSXjgTKU"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Bernoulli Lambda: {best_lambda_bn}\")\n",
        "print(f\"Best Bernoulli Validation Accuracy: {best_accuracy_bn:.2f}%\")"
      ],
      "metadata": {
        "id": "Tv7N7XuhgaHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89781d68-b2dd-4e36-9b17-688e4a85ce71"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Bernoulli Lambda: 0\n",
            "Best Bernoulli Validation Accuracy: 96.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def evaluate_bag_of_words_logistic_regression_on_dataset(train_df, test_df, eta, best_lambda_bow, num_iterations, dataset_name):\n",
        "    # Fit the vectorizer on the entire training set (raw text)\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_full_bow = vectorizer.fit_transform(train_df['Message'].values)\n",
        "    y_train_full = train_df['Category'].values\n",
        "\n",
        "    # Train logistic regression model with the best lambda for Bag of Words\n",
        "    weights_bow_full = mcap_logistic_regression_train(X_train_full_bow, y_train_full, eta, best_lambda_bow, num_iterations)\n",
        "\n",
        "    # Transform test data\n",
        "    X_test_bow = vectorizer.transform(test_df['Message'].values)\n",
        "    y_test = test_df['Category'].values\n",
        "\n",
        "    # Get predictions and calculate accuracy on test set\n",
        "    test_predictions_bow = mcap_logistic_regression_test(X_test_bow, weights_bow_full)\n",
        "    test_accuracy_bow = np.mean(test_predictions_bow == y_test) * 100\n",
        "\n",
        "    print(f\"Final Test Accuracy (Logistic regression Bag of Words) on {dataset_name}: {test_accuracy_bow:.2f}%\")"
      ],
      "metadata": {
        "id": "ge-sQa46umN7"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bag_of_words_logistic_regression_on_dataset(train_df, test_df, eta, best_lambda_bow, num_iterations, 'Test Dataset')"
      ],
      "metadata": {
        "id": "TTXvNpxAA0gM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98b4a52-ec96-4461-9f90-a368efa529d4"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy (Logistic regression Bag of Words) on Test Dataset: 95.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bernoulli_logistic_regression_on_dataset(train_df, test_df, eta, best_lambda_bn, num_iterations, dataset_name):\n",
        "    # Fit the vectorizer on the entire training set (raw text) for Bernoulli\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    X_train_full_bn = vectorizer.fit_transform(train_df['Message'].values)\n",
        "    y_train_full = train_df['Category'].values\n",
        "\n",
        "    # Train logistic regression model with the best lambda for Bernoulli\n",
        "    weights_bn_full = mcap_logistic_regression_train(X_train_full_bn, y_train_full, eta, best_lambda_bn, num_iterations)\n",
        "\n",
        "    # Transform test data\n",
        "    X_test_bn = vectorizer.transform(test_df['Message'].values)\n",
        "    y_test = test_df['Category'].values\n",
        "\n",
        "    # Get predictions and calculate accuracy on test set\n",
        "    test_predictions_bn = mcap_logistic_regression_test(X_test_bn, weights_bn_full)\n",
        "    test_accuracy_bn = np.mean(test_predictions_bn == y_test) * 100\n",
        "\n",
        "    print(f\"Final Test Accuracy (Logistic regression Bernoulli) on {dataset_name}: {test_accuracy_bn:.2f}%\")"
      ],
      "metadata": {
        "id": "UC0KqLlnum6o"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bernoulli_logistic_regression_on_dataset(train_df, test_df, eta, best_lambda_bn, num_iterations, 'Test Dataset')"
      ],
      "metadata": {
        "id": "iwl5-uOVCbDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e1f54b-3378-48b5-e550-fe890c80af31"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy (Logistic regression Bernoulli) on Test Dataset: 96.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SGD Classifier**"
      ],
      "metadata": {
        "id": "jTQru10FVLSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qEUU0gBVVP7D"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train_df into 70% train and 30% validation\n",
        "train_split, validation_split = train_test_split(train_df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "tLRBWNPOlnT4"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize text using CountVectorizer for Bag of Words\n",
        "vectorizer_bow = CountVectorizer(min_df=2)\n",
        "X_train_bow = vectorizer_bow.fit_transform(train_split['Message'])\n",
        "X_validation_bow = vectorizer_bow.transform(validation_split['Message'])"
      ],
      "metadata": {
        "id": "5hU-8Dwdk9zQ"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_bow = train_split['Category']\n",
        "y_validation_bow = validation_split['Category']"
      ],
      "metadata": {
        "id": "3pYBdAQKlEok"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize text using CountVectorizer for Bernoulli\n",
        "vectorizer_bn = CountVectorizer(binary=True, min_df=2)\n",
        "X_train_bn = vectorizer_bn.fit_transform(train_split['Message'])\n",
        "X_validation_bn = vectorizer_bn.transform(validation_split['Message'])"
      ],
      "metadata": {
        "id": "1n6C50akloRe"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_bn = train_split['Category']\n",
        "y_validation_bn = validation_split['Category']"
      ],
      "metadata": {
        "id": "VXfPE_Nflra1"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.001, 0.01, 0.1],\n",
        "    'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
        "    'eta0': [0.001, 0.01, 0.1]\n",
        "}"
      ],
      "metadata": {
        "id": "HwQ75tJPkkDW"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SGDClassifier\n",
        "sgd_classifier = SGDClassifier()\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(sgd_classifier, param_grid, cv=5, n_jobs=-1, scoring='accuracy')"
      ],
      "metadata": {
        "id": "LVSGgfp-k0g9"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit GridSearchCV for Bag of Words\n",
        "grid_search.fit(X_train_bow, y_train_bow)\n",
        "best_sgd_bow = grid_search.best_estimator_\n",
        "print(f\"Best parameters for Bag of Words: {grid_search.best_params_}\")\n",
        "\n",
        "# Predict and evaluate for Bag of Words\n",
        "y_pred_bow = best_sgd_bow.predict(X_validation_bow)\n",
        "accuracy_bow = accuracy_score(y_validation_bow, y_pred_bow)\n",
        "print(f\"Validation Accuracy for (SGD Classifier for Bag of Words): {accuracy_bow * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "L2q34Doyk2zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72815406-7152-43a8-bcef-f6eace08eedc"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Bag of Words: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log_loss', 'penalty': 'l2'}\n",
            "Validation Accuracy for (SGD Classifier for Bag of Words): 95.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit GridSearchCV for Bernoulli\n",
        "grid_search.fit(X_train_bn, y_train_bn)\n",
        "best_sgd_bernoulli = grid_search.best_estimator_\n",
        "print(f\"Best parameters for Bernoulli: {grid_search.best_params_}\")\n",
        "\n",
        "# Predict and evaluate for Bernoulli\n",
        "y_pred_bernoulli = best_sgd_bernoulli.predict(X_validation_bn)\n",
        "accuracy_bernoulli = accuracy_score(y_validation_bn, y_pred_bernoulli)\n",
        "print(f\"Validation Accuracy for (SGD Classifier for Bernoulli): {accuracy_bernoulli * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "713bvWlIk5Wq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1119ce5b-0234-4bef-b391-ba1d3f93d4d4"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Bernoulli: {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
            "Validation Accuracy for (SGD Classifier for Bernoulli): 96.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Calculation Functions**"
      ],
      "metadata": {
        "id": "o_gVbQIi6v09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "ufgSc6gqpcqb"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multinomial_naive_bayes_metrics(test_df, prior, conditional_probability, vocabulary, dataset_name):\n",
        "    # Call predict function\n",
        "    predictions = predict_bag_of_words_test_data(test_df, prior, conditional_probability, vocabulary)\n",
        "\n",
        "    # Extract true labels\n",
        "    true_labels = test_df['Category']\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predictions) * 100\n",
        "    precision = precision_score(true_labels, predictions) * 100\n",
        "    recall = recall_score(true_labels, predictions) * 100\n",
        "    f1 = f1_score(true_labels, predictions) * 100\n",
        "\n",
        "    # Print the accuracy in the same format\n",
        "    print(f\"Metrics for {dataset_name} (Multinomial Naive Bayes Bag of Words):\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}%\")\n",
        "    print(f\"Recall: {recall:.2f}%\")\n",
        "    print(f\"F1 Score: {f1:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "q-IoyhAv1i7F"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_multinomial_naive_bayes_metrics(enron1_df, prior, conditional_probability, vocabulary, 'Enron1 Dataset')\n",
        "evaluate_multinomial_naive_bayes_metrics(enron2_df, prior, conditional_probability, vocabulary, 'Enron2 Dataset')\n",
        "evaluate_multinomial_naive_bayes_metrics(enron4_df, prior, conditional_probability, vocabulary, 'Enron4 Dataset')"
      ],
      "metadata": {
        "id": "sJvE0WnUyT3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f625d080-da58-41ad-cf67-715301bf0bb1"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Enron1 Dataset (Multinomial Naive Bayes Bag of Words):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 94.74%\n",
            "Precision: 96.30%\n",
            "Recall: 87.25%\n",
            "F1 Score: 91.55%\n",
            "\n",
            "Metrics for Enron2 Dataset (Multinomial Naive Bayes Bag of Words):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 96.86%\n",
            "Precision: 97.52%\n",
            "Recall: 90.77%\n",
            "F1 Score: 94.02%\n",
            "\n",
            "Metrics for Enron4 Dataset (Multinomial Naive Bayes Bag of Words):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 92.82%\n",
            "Precision: 99.72%\n",
            "Recall: 90.28%\n",
            "F1 Score: 94.77%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_discrete_NB(test_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary, dataset_name):\n",
        "    # Call predict function\n",
        "    predictions = predict_bernoulli_test_data(test_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary)\n",
        "\n",
        "    # Extract true labels\n",
        "    true_labels = test_df['Category']\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(true_labels, predictions) * 100\n",
        "    precision = precision_score(true_labels, predictions) * 100\n",
        "    recall = recall_score(true_labels, predictions) * 100\n",
        "    f1 = f1_score(true_labels, predictions) * 100\n",
        "\n",
        "    # Print the metrics in the desired format\n",
        "    print(f\"Metrics for {dataset_name} (Discrete Naive Bayes Bernoulli):\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}%\")\n",
        "    print(f\"Recall: {recall:.2f}%\")\n",
        "    print(f\"F1 Score: {f1:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "4sDSJQRvUZXy"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_discrete_NB(enron1_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary, 'Enron1 Dataset')\n",
        "evaluate_discrete_NB(enron2_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary, 'Enron2 Dataset')\n",
        "evaluate_discrete_NB(enron4_df, prior, conditional_probability, conditional_probability_of_not_seen_word, vocabulary, 'Enron4 Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlCDHwoSUnwj",
        "outputId": "ef7797b7-2720-43ac-f3af-23cca0b2164d"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Enron1 Dataset (Discrete Naive Bayes Bernoulli):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 95.61%\n",
            "Precision: 95.10%\n",
            "Recall: 91.28%\n",
            "F1 Score: 93.15%\n",
            "\n",
            "Metrics for Enron2 Dataset (Discrete Naive Bayes Bernoulli):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 97.28%\n",
            "Precision: 95.35%\n",
            "Recall: 94.62%\n",
            "F1 Score: 94.98%\n",
            "\n",
            "Metrics for Enron4 Dataset (Discrete Naive Bayes Bernoulli):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 95.03%\n",
            "Precision: 100.00%\n",
            "Recall: 93.09%\n",
            "F1 Score: 96.42%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bag_of_words_LR(train_df, test_df, eta, best_lambda_bow, num_iterations, dataset_name):\n",
        "    # Fit the vectorizer on the entire training set (raw text)\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_full_bow = vectorizer.fit_transform(train_df['Message'].values)\n",
        "    y_train_full = train_df['Category'].values\n",
        "\n",
        "    # Train logistic regression model with the best lambda for Bag of Words\n",
        "    weights_bow_full = mcap_logistic_regression_train(X_train_full_bow, y_train_full, eta, best_lambda_bow, num_iterations)\n",
        "\n",
        "    # Transform test data\n",
        "    X_test_bow = vectorizer.transform(test_df['Message'].values)\n",
        "    y_test = test_df['Category'].values\n",
        "\n",
        "    # Get predictions\n",
        "    test_predictions_bow = mcap_logistic_regression_test(X_test_bow, weights_bow_full)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(y_test, test_predictions_bow) * 100\n",
        "    precision = precision_score(y_test, test_predictions_bow) * 100\n",
        "    recall = recall_score(y_test, test_predictions_bow) * 100\n",
        "    f1 = f1_score(y_test, test_predictions_bow) * 100\n",
        "\n",
        "    # Print the metrics in the desired format\n",
        "    print(f\"Metrics for {dataset_name} (Logistic Regression Bag of Words):\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}%\")\n",
        "    print(f\"Recall: {recall:.2f}%\")\n",
        "    print(f\"F1 Score: {f1:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "m1ZnP1ayUssQ"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bag_of_words_LR(train_df, enron1_df, eta, best_lambda_bow, num_iterations, 'Enron1 Dataset')\n",
        "evaluate_bag_of_words_LR(train_df, enron2_df, eta, best_lambda_bow, num_iterations, 'Enron2 Dataset')\n",
        "evaluate_bag_of_words_LR(train_df, enron4_df, eta, best_lambda_bow, num_iterations, 'Enron4 Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1-uVofOVDK6",
        "outputId": "06bb5425-d032-48c7-e59d-e72e45693043"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Enron1 Dataset (Logistic Regression Bag of Words):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 96.05%\n",
            "Precision: 89.70%\n",
            "Recall: 99.33%\n",
            "F1 Score: 94.27%\n",
            "\n",
            "Metrics for Enron2 Dataset (Logistic Regression Bag of Words):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 94.35%\n",
            "Precision: 86.01%\n",
            "Recall: 94.62%\n",
            "F1 Score: 90.11%\n",
            "\n",
            "Metrics for Enron4 Dataset (Logistic Regression Bag of Words):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 96.32%\n",
            "Precision: 98.94%\n",
            "Recall: 95.91%\n",
            "F1 Score: 97.40%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bernoulli_LR(train_df, test_df, eta, best_lambda_bn, num_iterations, dataset_name):\n",
        "    # Fit the vectorizer on the entire training set (raw text) for Bernoulli\n",
        "    vectorizer = CountVectorizer(binary=True)\n",
        "    X_train_full_bn = vectorizer.fit_transform(train_df['Message'].values)\n",
        "    y_train_full = train_df['Category'].values\n",
        "\n",
        "    # Train logistic regression model with the best lambda for Bernoulli\n",
        "    weights_bn_full = mcap_logistic_regression_train(X_train_full_bn, y_train_full, eta, best_lambda_bn, num_iterations)\n",
        "\n",
        "    # Transform test data\n",
        "    X_test_bn = vectorizer.transform(test_df['Message'].values)\n",
        "    y_test = test_df['Category'].values\n",
        "\n",
        "    # Get predictions\n",
        "    test_predictions_bn = mcap_logistic_regression_test(X_test_bn, weights_bn_full)\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(y_test, test_predictions_bn) * 100\n",
        "    precision = precision_score(y_test, test_predictions_bn) * 100\n",
        "    recall = recall_score(y_test, test_predictions_bn) * 100\n",
        "    f1 = f1_score(y_test, test_predictions_bn) * 100\n",
        "\n",
        "    # Print the metrics in the desired format\n",
        "    print(f\"Metrics for {dataset_name} (Logistic Regression Bernoulli):\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.2f}%\")\n",
        "    print(f\"Recall: {recall:.2f}%\")\n",
        "    print(f\"F1 Score: {f1:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ITUA7FKUVM1E"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_bernoulli_LR(train_df, enron1_df, eta, best_lambda_bn, num_iterations, 'Enron1 Dataset')\n",
        "evaluate_bernoulli_LR(train_df, enron2_df, eta, best_lambda_bn, num_iterations, 'Enron2 Dataset')\n",
        "evaluate_bernoulli_LR(train_df, enron4_df, eta, best_lambda_bn, num_iterations, 'Enron4 Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L6TF3nGVQOV",
        "outputId": "05fd8633-e49a-4d50-c694-ecfb7fa9887a"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics for Enron1 Dataset (Logistic Regression Bernoulli):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 95.83%\n",
            "Precision: 89.63%\n",
            "Recall: 98.66%\n",
            "F1 Score: 93.93%\n",
            "\n",
            "Metrics for Enron2 Dataset (Logistic Regression Bernoulli):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 96.23%\n",
            "Precision: 89.44%\n",
            "Recall: 97.69%\n",
            "F1 Score: 93.38%\n",
            "\n",
            "Metrics for Enron4 Dataset (Logistic Regression Bernoulli):\n",
            "---------------------------------------------------------------\n",
            "Accuracy: 97.24%\n",
            "Precision: 98.96%\n",
            "Recall: 97.19%\n",
            "F1 Score: 98.06%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.001, 0.01, 0.1],\n",
        "    'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
        "    'eta0': [0.001, 0.01, 0.1]\n",
        "}"
      ],
      "metadata": {
        "id": "hAh1twOsXNiN"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_bag_of_words_SGD(train_df, test_df, param_grid, dataset_name):\n",
        "    # Preprocess data and vectorize using Bag of Words\n",
        "    vectorizer_bow = CountVectorizer(min_df=2)\n",
        "    X_train_bow = vectorizer_bow.fit_transform(train_df['Message'])\n",
        "    X_test_bow = vectorizer_bow.transform(test_df['Message'])\n",
        "\n",
        "    y_train = train_df['Category']\n",
        "    y_test = test_df['Category']\n",
        "\n",
        "    # Define SGDClassifier and perform grid search\n",
        "    sgd_classifier = SGDClassifier()\n",
        "    grid_search = GridSearchCV(sgd_classifier, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_bow, y_train)\n",
        "\n",
        "    best_sgd_bow = grid_search.best_estimator_\n",
        "    print(f\"Best parameters for Bag of Words: {grid_search.best_params_}\")\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = best_sgd_bow.predict(X_test_bow)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy_bow = accuracy_score(y_test, y_pred) * 100\n",
        "    precision_bow = precision_score(y_test, y_pred) * 100\n",
        "    recall_bow = recall_score(y_test, y_pred) * 100\n",
        "    f1_bow = f1_score(y_test, y_pred) * 100\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Metrics for {dataset_name} (SGD Classifier - Bag of Words):\")\n",
        "    print(\"--------------------------------------------------------------\")\n",
        "    print(f\"Accuracy: {accuracy_bow:.2f}%\")\n",
        "    print(f\"Precision: {precision_bow:.2f}%\")\n",
        "    print(f\"Recall: {recall_bow:.2f}%\")\n",
        "    print(f\"F1 Score: {f1_bow:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "zseOn4vhVT4P"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Bag of Words Model\n",
        "evaluate_bag_of_words_SGD(train_df, enron1_df, param_grid, 'Enron1 Dataset')\n",
        "evaluate_bag_of_words_SGD(train_df, enron2_df, param_grid, 'Enron2 Dataset')\n",
        "evaluate_bag_of_words_SGD(train_df, enron4_df, param_grid, 'Enron4 Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h9hR7a3XRSq",
        "outputId": "10773b51-069b-4862-f836-f32e09e7b334"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Bag of Words: {'alpha': 0.1, 'eta0': 0.001, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
            "Metrics for Enron1 Dataset (SGD Classifier - Bag of Words):\n",
            "--------------------------------------------------------------\n",
            "Accuracy: 95.83%\n",
            "Precision: 90.12%\n",
            "Recall: 97.99%\n",
            "F1 Score: 93.89%\n",
            "\n",
            "Best parameters for Bag of Words: {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log_loss', 'penalty': 'l2'}\n",
            "Metrics for Enron2 Dataset (SGD Classifier - Bag of Words):\n",
            "--------------------------------------------------------------\n",
            "Accuracy: 94.98%\n",
            "Precision: 85.81%\n",
            "Recall: 97.69%\n",
            "F1 Score: 91.37%\n",
            "\n",
            "Best parameters for Bag of Words: {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
            "Metrics for Enron4 Dataset (SGD Classifier - Bag of Words):\n",
            "--------------------------------------------------------------\n",
            "Accuracy: 97.79%\n",
            "Precision: 98.97%\n",
            "Recall: 97.95%\n",
            "F1 Score: 98.46%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bernoulli_SGD(train_df, test_df, param_grid, dataset_name):\n",
        "    # Preprocess data and vectorize using Bernoulli\n",
        "    vectorizer_bn = CountVectorizer(binary=True, min_df=2)\n",
        "    X_train_bn = vectorizer_bn.fit_transform(train_df['Message'])\n",
        "    X_test_bn = vectorizer_bn.transform(test_df['Message'])\n",
        "\n",
        "    y_train = train_df['Category']\n",
        "    y_test = test_df['Category']\n",
        "\n",
        "    # Define SGDClassifier and perform grid search\n",
        "    sgd_classifier = SGDClassifier()\n",
        "    grid_search = GridSearchCV(sgd_classifier, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "    grid_search.fit(X_train_bn, y_train)\n",
        "\n",
        "    best_sgd_bn = grid_search.best_estimator_\n",
        "    print(f\"Best parameters for Bernoulli: {grid_search.best_params_}\")\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = best_sgd_bn.predict(X_test_bn)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy_bn = accuracy_score(y_test, y_pred) * 100\n",
        "    precision_bn = precision_score(y_test, y_pred) * 100\n",
        "    recall_bn = recall_score(y_test, y_pred) * 100\n",
        "    f1_bn = f1_score(y_test, y_pred) * 100\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Metrics for {dataset_name} (SGD Classifier - Bernoulli):\")\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    print(f\"Accuracy: {accuracy_bn:.2f}%\")\n",
        "    print(f\"Precision: {precision_bn:.2f}%\")\n",
        "    print(f\"Recall: {recall_bn:.2f}%\")\n",
        "    print(f\"F1 Score: {f1_bn:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ps5mhxYQXLW_"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Bernoulli Model\n",
        "evaluate_bernoulli_SGD(train_df, enron1_df, param_grid, 'Enron1 Dataset')\n",
        "evaluate_bernoulli_SGD(train_df, enron2_df, param_grid, 'Enron2 Dataset')\n",
        "evaluate_bernoulli_SGD(train_df, enron4_df, param_grid, 'Enron4 Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbgCg4zYXUID",
        "outputId": "92f7d7e8-d316-4746-d0d5-5a300ca43735"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Bernoulli: {'alpha': 0.01, 'eta0': 0.001, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
            "Metrics for Enron1 Dataset (SGD Classifier - Bernoulli):\n",
            "------------------------------------------------------------\n",
            "Accuracy: 96.49%\n",
            "Precision: 92.36%\n",
            "Recall: 97.32%\n",
            "F1 Score: 94.77%\n",
            "\n",
            "Best parameters for Bernoulli: {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
            "Metrics for Enron2 Dataset (SGD Classifier - Bernoulli):\n",
            "------------------------------------------------------------\n",
            "Accuracy: 96.65%\n",
            "Precision: 90.14%\n",
            "Recall: 98.46%\n",
            "F1 Score: 94.12%\n",
            "\n",
            "Best parameters for Bernoulli: {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log_loss', 'penalty': 'l2'}\n",
            "Metrics for Enron4 Dataset (SGD Classifier - Bernoulli):\n",
            "------------------------------------------------------------\n",
            "Accuracy: 98.90%\n",
            "Precision: 99.49%\n",
            "Recall: 98.98%\n",
            "F1 Score: 99.23%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}